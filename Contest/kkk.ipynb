{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!pip install transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "home_dir = \"gdrive/My Drive/propaganda_detection\"\n",
    "data_dir = os.path.join(home_dir, \"datasets\")\n",
    "model_dir = os.path.join(home_dir, \"model_dir\")\n",
    "if not os.path.isdir(model_dir):\n",
    "  os.mkdir(model_dir)\n",
    "\n",
    "\n",
    "\n",
    "  # Read training articles\n",
    "def read_articles(article_dir):\n",
    "  articles = []\n",
    "  train_dir = os.path.join(data_dir, article_dir)\n",
    "  for filename in sorted(os.listdir(train_dir)):\n",
    "    myfile = open(os.path.join(train_dir, filename))\n",
    "    article = myfile.read()\n",
    "    articles.append(article)\n",
    "    myfile.close()\n",
    "  article_ids = []\n",
    "  for filename in sorted(os.listdir(train_dir)):\n",
    "    article_ids.append(filename[7:-4])\n",
    "  return articles, article_ids\n",
    "\n",
    "\n",
    "\n",
    "# Read training span labels \n",
    "def read_spans(span_dir=None):\n",
    "  spans = []\n",
    "  if span_dir is None:\n",
    "    label_dir = os.path.join(data_dir, \"train-labels-task1-span-identification\")\n",
    "  else:\n",
    "    label_dir = os.path.join(data_dir, span_dir)\n",
    "  for filename in sorted(os.listdir(label_dir)):\n",
    "    myfile = open(os.path.join(label_dir, filename))\n",
    "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
    "    span = []\n",
    "    for row in tsvreader:\n",
    "      span.append((int(row[1]), int(row[2])))\n",
    "    myfile.close()\n",
    "    spans.append(span)\n",
    "  return spans\n",
    "\n",
    "\n",
    "def print_spans(article, span):\n",
    "  for sp in span:\n",
    "    print (article[sp[0]: sp[1]])\n",
    "  print()\n",
    "\n",
    "\n",
    "  class example_sentence:\n",
    "  def __init__(self):\n",
    "    self.tokens = []\n",
    "    self.labels = []\n",
    "    self.article_index = -1 # index of the article to which the sentence is associated\n",
    "    self.index = -1 # index of the sentence in that article \n",
    "    self.word_to_start_char_offset = []\n",
    "    self.word_to_end_char_offset = []\n",
    "  \n",
    "  def __str__(self):\n",
    "    print(\"tokens -\", self.tokens)\n",
    "    print(\"labels -\", self.labels)\n",
    "    print(\"article_index -\", self.article_index)\n",
    "    print(\"index -\", self.index)\n",
    "    print(\"start_offset -\", self.word_to_start_char_offset)\n",
    "    print(\"end_offset -\", self.word_to_end_char_offset)   \n",
    "    return \"\"    \n",
    "  \n",
    "\n",
    "  def is_whitespace(c):\n",
    "  if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def get_sentence_tokens_labels(article, span=None, article_index=None):\n",
    "  doc_tokens = []\n",
    "  char_to_word_offset = []\n",
    "  current_sentence_tokens = [] # actually all sentence tokens for particular article. #TODO rename\n",
    "  word_to_start_char_offset = {}\n",
    "  word_to_end_char_offset = {}\n",
    "  prev_is_whitespace = True\n",
    "  prev_is_newline = True\n",
    "  current_word_position = None\n",
    "  for index, c in enumerate(article):\n",
    "    if c == \"\\n\":\n",
    "      prev_is_newline = True\n",
    "      # check for empty lists\n",
    "      if doc_tokens:\n",
    "        current_sentence_tokens.append(doc_tokens)\n",
    "      doc_tokens = []\n",
    "    if is_whitespace(c):\n",
    "      prev_is_whitespace = True\n",
    "      if current_word_position is not None:\n",
    "        word_to_end_char_offset[current_word_position] = index\n",
    "        current_word_position = None\n",
    "    else:\n",
    "      if prev_is_whitespace:\n",
    "        doc_tokens.append(c)\n",
    "        current_word_position = (len(current_sentence_tokens), len(doc_tokens) - 1)\n",
    "        word_to_start_char_offset[current_word_position] = index # start offset of word\n",
    "      else:\n",
    "        doc_tokens[-1] += c\n",
    "      prev_is_whitespace = False\n",
    "    char_to_word_offset.append((len(current_sentence_tokens), len(doc_tokens) - 1))\n",
    "  if doc_tokens:\n",
    "    current_sentence_tokens.append(doc_tokens)\n",
    "  if current_word_position is not None:\n",
    "    word_to_end_char_offset[current_word_position] = index\n",
    "    current_word_position = None\n",
    "  if span is None:\n",
    "    return current_sentence_tokens, (word_to_start_char_offset, word_to_end_char_offset)\n",
    "\n",
    "  current_propaganda_labels = []\n",
    "  for doc_tokens in current_sentence_tokens:\n",
    "    current_propaganda_labels.append([0] * len(doc_tokens))\n",
    "\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "\n",
    "  for sp in span:\n",
    "    if (char_to_word_offset[sp[0]][0] != char_to_word_offset[sp[1]-1][0]):\n",
    "      l1 = char_to_word_offset[sp[0]][0]\n",
    "      l2 = char_to_word_offset[sp[1] - 1][0]\n",
    "      start_positions.append(char_to_word_offset[sp[0]])\n",
    "      end_positions.append((l1, len(current_sentence_tokens[l1])-1))\n",
    "      l1 += 1\n",
    "      while(l1 < l2):\n",
    "        start_positions.append((l1, 0))\n",
    "        end_positions.append((l1, len(current_sentence_tokens[l1])-1))\n",
    "        l1 += 1\n",
    "      start_positions.append((l2, 0))\n",
    "      end_positions.append(char_to_word_offset[sp[1]-1])  \n",
    "      continue\n",
    "    start_positions.append(char_to_word_offset[sp[0]])\n",
    "    end_positions.append(char_to_word_offset[sp[1]-1])\n",
    "\n",
    "  for i, s in enumerate(start_positions):\n",
    "    assert start_positions[i][0] == end_positions[i][0]\n",
    "    if TAGGING_SCHEME == \"BIO\":\n",
    "      current_propaganda_labels[start_positions[i][0]][start_positions[i][1]] = 2 # Begin label\n",
    "      if start_positions[i][1] < end_positions[i][1]:\n",
    "        current_propaganda_labels[start_positions[i][0]][start_positions[i][1] + 1 : end_positions[i][1] + 1] = [1] * (end_positions[i][1] - start_positions[i][1])\n",
    "    if TAGGING_SCHEME == \"BIOE\":\n",
    "      current_propaganda_labels[start_positions[i][0]][start_positions[i][1]] = 2 # Begin label\n",
    "      if start_positions[i][1] < end_positions[i][1]:\n",
    "        current_propaganda_labels[start_positions[i][0]][start_positions[i][1] + 1 : end_positions[i][1]] = [1] * (end_positions[i][1] - start_positions[i][1] - 1)\n",
    "        current_propaganda_labels[start_positions[i][0]][end_positions[i][1]] = 3 # End label\n",
    "    else:\n",
    "      current_propaganda_labels[start_positions[i][0]][start_positions[i][1] : end_positions[i][1] + 1] = [1] * (end_positions[i][1] + 1 - start_positions[i][1])\n",
    "  \n",
    "  num_sentences = len(current_sentence_tokens)\n",
    "\n",
    "  start_offset_list = get_list_from_dict(num_sentences, word_to_start_char_offset)\n",
    "  end_offset_list = get_list_from_dict(num_sentences, word_to_end_char_offset)\n",
    "  sentences = []\n",
    "  for i in range(num_sentences):\n",
    "    sentence = example_sentence()\n",
    "    sentence.tokens = current_sentence_tokens[i]\n",
    "    sentence.labels = current_propaganda_labels[i]\n",
    "    sentence.article_index =  article_index\n",
    "    sentence.index = i\n",
    "    sentence.word_to_start_char_offset = start_offset_list[i]\n",
    "    sentence.word_to_end_char_offset = end_offset_list[i]\n",
    "    num_words = len(sentence.tokens)\n",
    "    assert len(sentence.labels) == num_words\n",
    "    assert len(sentence.word_to_start_char_offset) == num_words\n",
    "    assert len(sentence.word_to_end_char_offset) == num_words\n",
    "    sentences.append(sentence)\n",
    "\n",
    "  return current_sentence_tokens, current_propaganda_labels, (word_to_start_char_offset, word_to_end_char_offset), sentences\n",
    "\n",
    "\n",
    "def get_list_from_dict(num_sentences, word_offsets):\n",
    "  li = []\n",
    "  for _ in range(num_sentences):\n",
    "    li.append([])\n",
    "  for key in word_offsets:\n",
    "    si = key[0]\n",
    "    li[si].append(word_offsets[key])\n",
    "\n",
    "  return li\n",
    "\n",
    "\n",
    "class BertExample:\n",
    "  def __init__(self):\n",
    "    self.add_cls_sep = True\n",
    "    self.sentence_id = -1\n",
    "    self.orig_to_tok_index = []\n",
    "    self.tok_to_orig_index = []\n",
    "    self.labels = None\n",
    "    self.tokens_ids = []\n",
    "    self.input_mask = []\n",
    "  def __str__(self):\n",
    "    print(\"sentence_id\", self.sentence_id)\n",
    "    return \"\"\n",
    "  \n",
    "\n",
    "def convert_sentence_to_input_feature(sentence, sentence_id, tokenizer, add_cls_sep=True, max_seq_len=256):\n",
    "  bert_example = BertExample()\n",
    "  bert_example.sentence_id = sentence_id\n",
    "  bert_example.add_cls_sep = add_cls_sep\n",
    "\n",
    "  sentence_tokens = sentence.tokens\n",
    "  sentence_labels = sentence.labels \n",
    "\n",
    "  tok_to_orig_index = []\n",
    "  orig_to_tok_index = []\n",
    "  all_doc_tokens = [] \n",
    "  for (i, token) in enumerate(sentence_tokens):\n",
    "    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "    sub_tokens = tokenizer.tokenize(token)\n",
    "    for sub_token in sub_tokens:\n",
    "      tok_to_orig_index.append(i)\n",
    "      all_doc_tokens.append(sub_token)\n",
    "  bert_example.tok_to_orig_index = tok_to_orig_index\n",
    "  bert_example.orig_to_tok_index = orig_to_tok_index\n",
    "\n",
    "  bert_tokens = all_doc_tokens\n",
    "  if add_cls_sep:\n",
    "    bert_tokens = [\"[CLS]\"] + bert_tokens\n",
    "    bert_tokens = bert_tokens + [\"[SEP]\"]\n",
    "  \n",
    "  tokens_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "  input_mask = [1] * len(tokens_ids)\n",
    "  while len(tokens_ids) < max_seq_len:\n",
    "    tokens_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "  bert_example.tokens_ids = tokens_ids\n",
    "  bert_example.input_mask = input_mask\n",
    "\n",
    "  if sentence_labels is None:\n",
    "    return bert_example\n",
    "\n",
    "  labels = [0] * len(all_doc_tokens)\n",
    "  for index, token in enumerate(all_doc_tokens):\n",
    "    labels[index] = sentence_labels[tok_to_orig_index[index]]\n",
    "  if add_cls_sep:\n",
    "    labels = [0] + labels\n",
    "    labels = labels + [0]\n",
    "  while len(labels) < max_seq_len:\n",
    "    labels.append(0)\n",
    "  bert_example.labels = labels\n",
    "\n",
    "  return bert_example \n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def get_dataloader(examples, batch_size=8):\n",
    "  inputs = torch.tensor([d.tokens_ids for d in examples])\n",
    "  labels = torch.tensor([d.labels for d in examples])\n",
    "  masks = torch.tensor([d.input_mask for d in examples])\n",
    "  sentence_ids = torch.tensor([d.sentence_id for d in examples])\n",
    "  tensor_data = TensorDataset(inputs, labels, masks, sentence_ids)\n",
    "  dataloader = DataLoader(tensor_data, batch_size=BATCH_SIZE)\n",
    "  return dataloader\n",
    "\n",
    "def get_data(articles, spans, indices):\n",
    "  assert len(articles) == len(spans)    \n",
    "  sentences = []\n",
    "  for index in indices:\n",
    "    article = articles[index]\n",
    "    span = spans[index]\n",
    "    _, _, _, cur_sentences = get_sentence_tokens_labels(article, span, index)\n",
    "    sentences += cur_sentences\n",
    "  print(len(sentences))\n",
    "  print(max([len(s.tokens) for s in sentences]))\n",
    "  bert_examples = []\n",
    "  for i, sentence in enumerate(sentences):\n",
    "    input_feature = convert_sentence_to_input_feature(sentence, i, tokenizer)\n",
    "    bert_examples.append(input_feature)\n",
    "  dataloader = get_dataloader(bert_examples, BATCH_SIZE)\n",
    "  return dataloader, sentences, bert_examples\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "  pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "'''\n",
    "  Custom classification head on top of BERT LM.\n",
    "'''\n",
    "class CustomBertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.my_hidden_size = 128\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # self.my_hidden = nn.Linear(config.hidden_size, self.my_hidden_size)\n",
    "        # self.classifier = nn.Linear(self.my_hidden_size, config.num_labels)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # logits = self.my_hidden(sequence_output)\n",
    "        # logits = self.classifier(logits)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(weight=WEIGHTS)\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, epochs=5, save_model=False):\n",
    "  max_grad_norm = 1.0\n",
    "\n",
    "  for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "      # add batch to gpu\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      b_input_ids, b_labels, b_input_mask, b_ids = batch\n",
    "      loss, _ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "      loss.backward()\n",
    "      tr_loss += loss.item()\n",
    "      nb_tr_examples += b_input_ids.size(0)\n",
    "      nb_tr_steps += 1\n",
    "      torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "      optimizer.step()\n",
    "      model.zero_grad()\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "    get_score(model, mode=\"train\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in eval_dataloader:\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      b_input_ids, b_labels, b_input_mask, b_ids = batch\n",
    "      with torch.no_grad():\n",
    "        tmp_eval_loss, _ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    " \n",
    "      eval_loss += tmp_eval_loss.mean().item()\n",
    "      \n",
    "      nb_eval_examples += b_input_ids.size(0)\n",
    "      nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "\n",
    "    get_score(model, mode=\"eval\")\n",
    "    if save_model:\n",
    "      model_name = 'model_' + str(datetime.datetime.now()) + '.pt'\n",
    "      torch.save(model, os.path.join(model_dir, model_name))\n",
    "      print(\"Model saved:\", model_name)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "def get_model_predictions(model, dataloader):\n",
    "  model.eval()\n",
    "  predictions , true_labels, sentence_ids = [], [], []\n",
    "  nb_eval_steps = 0\n",
    "  for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_labels, b_input_mask, b_ids = batch  \n",
    "    with torch.no_grad():\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = logits[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    s_ids = b_ids.to('cpu').numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "    sentence_ids.extend(s_ids)\n",
    "    nb_eval_steps += 1\n",
    "  \n",
    "  return predictions, true_labels, sentence_ids\n",
    "\n",
    "def merge_spans(current_spans):\n",
    "  if not current_spans:\n",
    "    return [] \n",
    "  merged_spans = []\n",
    "  li = current_spans[0][0]\n",
    "  ri = current_spans[0][1]\n",
    "  threshold = 2\n",
    "  for i in range(len(current_spans) - 1):\n",
    "    span = current_spans[i+1]\n",
    "    if span[0] - ri < 2:\n",
    "      ri = span[1]\n",
    "      continue\n",
    "    else:\n",
    "      merged_spans.append((li, ri))\n",
    "      li = span[0]\n",
    "      ri = span[1]\n",
    "  merged_spans.append((li, ri))\n",
    "  return merged_spans\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_dev_outputs(model, article_dir=\"dev-articles\"):\n",
    "  test_articles, test_article_ids = read_articles('dev-articles')\n",
    "  test_spans = [[]] * len(test_articles)\n",
    "  test_dataloader, test_sentences, test_bert_examples = get_data(test_articles, test_spans, indices=np.arange(len(test_articles)))\n",
    "  sps = get_score(model, mode=\"test\")\n",
    "  return sps\n",
    "\n",
    "\n",
    "from shutil import copyfile\n",
    "def get_score(model, mode=None):\n",
    "  predicted_spans = [[] for i in range(400)] # TODO 400 hardcoded\n",
    "  \n",
    "  def get_span_prediction(prediction_labels, sentence_index, sentences, bert_examples):\n",
    "    index = sentence_index \n",
    "    bert_example = bert_examples[index]\n",
    "    mask = bert_example.input_mask\n",
    "    pred_labels_masked = prediction_labels # need to change to predictions later\n",
    "    pred_labels = []\n",
    "    for i, m in enumerate(mask):\n",
    "      if m > 0:\n",
    "        pred_labels.append(pred_labels_masked[i])\n",
    "    if bert_example.add_cls_sep:\n",
    "      pred_labels.pop() # remove ['SEP'] label\n",
    "      pred_labels.pop(0) # remove ['CLS'] label\n",
    "\n",
    "    sentence = sentences[index]\n",
    "    sent_len = len(sentence.tokens)\n",
    "    final_pred_labels = [0] * sent_len\n",
    "    cur_map = bert_example.tok_to_orig_index\n",
    "    for i, label in enumerate(pred_labels):\n",
    "      final_pred_labels[cur_map[i]] |= label\n",
    "    # assert final_pred_labels == sentence.labels\n",
    "    \n",
    "    word_start_index_map = sentence.word_to_start_char_offset\n",
    "    word_end_index_map = sentence.word_to_end_char_offset\n",
    "\n",
    "    article_index = sentence.article_index\n",
    "    for i, label in enumerate(final_pred_labels):\n",
    "      if label:\n",
    "        predicted_spans[article_index].append((word_start_index_map[i], word_end_index_map[i]))\n",
    "  \n",
    "  if mode == \"train\":\n",
    "    indices = train_indices\n",
    "    predictions, true_labels, sentence_ids = get_model_predictions(model, train_dataloader)\n",
    "    pred_sentences, pred_bert_examples = train_sentences, train_bert_examples\n",
    "  elif mode == \"test\":\n",
    "    predictions, true_labels , sentence_ids = get_model_predictions(model, test_dataloader)\n",
    "    pred_sentences, pred_bert_examples = test_sentences, test_bert_examples\n",
    "  else:\n",
    "    indices = eval_indices\n",
    "    predictions, true_labels, sentence_ids = get_model_predictions(model, eval_dataloader)\n",
    "    pred_sentences, pred_bert_examples = eval_sentences, eval_bert_examples\n",
    "\n",
    "  merged_predicted_spans = []\n",
    "  # TODO sorting of spans???? may not be in order??\n",
    "  for ii, _ in enumerate(predictions):\n",
    "    get_span_prediction(predictions[ii], sentence_ids[ii], pred_sentences, pred_bert_examples)\n",
    "  for span in predicted_spans:\n",
    "    merged_predicted_spans.append(merge_spans(span))\n",
    "  if mode == \"test\":\n",
    "    return merged_predicted_spans \n",
    "  if not os.path.isdir(\"predictions\"):\n",
    "    os.mkdir(\"predictions\")\n",
    "  copyfile(\"gdrive/My Drive/propaganda_detection/tools/task-SI_scorer.py\", \"predictions/task-SI_scorer.py\")\n",
    "  with open(\"predictions/predictions.tsv\", 'w') as fp:\n",
    "    for index in indices:\n",
    "      filename = \"article\" + article_ids[index] + \".task1-SI.labels\"\n",
    "      copyfile(os.path.join(data_dir, \"train-labels-task1-span-identification/\" + filename), \"predictions/\" + filename)\n",
    "      for ii in merged_predicted_spans[index]:\n",
    "        fp.write(article_ids[index] + \"\\t\" + str(ii[0]) + \"\\t\" + str(ii[1]) + \"\\n\")\n",
    "\n",
    "  !python3 predictions/task-SI_scorer.py -s predictions/predictions.tsv -r predictions/ -m\n",
    "\n",
    "  for index in indices:\n",
    "    filename = \"article\" + article_ids[index] + \".task1-SI.labels\"\n",
    "    os.remove(\"predictions/\" + filename)\n",
    "\n",
    "\n",
    "\n",
    "articles, article_ids = read_articles('train-articles')\n",
    "spans = read_spans()\n",
    "NUM_ARTICLES = len(articles) # Change to run for smaller number of examples\n",
    "articles = articles[0:NUM_ARTICLES]\n",
    "spans = spans[0:NUM_ARTICLES]\n",
    "\n",
    "np.random.seed(245)\n",
    "indices = np.arange(NUM_ARTICLES)\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:int(0.9 * NUM_ARTICLES)]\n",
    "eval_indices = indices[int(0.9 * NUM_ARTICLES):]\n",
    "\n",
    "TAGGING_SCHEME = \"BIOE\"\n",
    "BATCH_SIZE=8\n",
    "bert_model_class = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_class, lower_case=True) # Change Tokenizer to run for some other class model\n",
    "\n",
    "train_dataloader, train_sentences, train_bert_examples = get_data(articles, spans, train_indices)\n",
    "eval_dataloader, eval_sentences, eval_bert_examples = get_data(articles, spans, eval_indices)\n",
    "\n",
    "\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "num_labels = 2 + int(TAGGING_SCHEME == \"BIO\") + 2 * int(TAGGING_SCHEME == \"BIOE\")\n",
    "# Change class to run other model eg: CustomBertForTokenClassification.from_pretrained(...)\n",
    "model = BertForTokenClassification.from_pretrained(bert_model_class, num_labels=num_labels) \n",
    "model.cuda()\n",
    "\n",
    "if TAGGING_SCHEME == \"BIOE\":\n",
    "  WEIGHTS = torch.tensor([1.0, 5.0, 10.0, 5.0]).cuda()\n",
    "else:\n",
    "  WEIGHTS = torch.tensor([1.0, 100.0]).cuda()\n",
    "\n",
    "epochs = 4\n",
    "total_steps = total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 3e-5,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, epochs=epochs, save_model=(NUM_ARTICLES >= 150))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = torch.load(os.path.join(model_dir, 'my_model.pt.pt')) # Test accuracy for custom model\n",
    "get_score(model, mode=\"train\")\n",
    "print()\n",
    "get_score(model, mode=\"eval\")\n",
    "\n",
    "\n",
    "\n",
    "# model = torch.load(os.path.join(model_dir, 'my_model.pt.pt')) # Test for custom model\n",
    "sps = get_dev_outputs(model)\n",
    "from google.colab import files\n",
    "with open('dev_predictions.txt', 'w') as fp:\n",
    "  for index in range(len(test_articles)):\n",
    "    for ii in sps[index]:\n",
    "      fp.write(test_article_ids[index] + \"\\t\" + str(ii[0]) + \"\\t\" + str(ii[1]) + \"\\n\")\n",
    "files.download('dev_predictions.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
